{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sivaratrisrinivas/ttt-playground/blob/main/notebooks/all_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTT Playground - All Tests\n",
    "\n",
    "Combined notebook for all phase tests. Sections:\n",
    "1. **Setup** - Clone, install, verify GPU\n",
    "2. **Phase 2** - Document Processing (PDF, Chunker, Validator)\n",
    "3. **Phase 3** - TTT-Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (or pull latest if exists)\n",
    "import os\n",
    "if os.path.exists('/content/ttt-playground'):\n",
    "    !cd /content/ttt-playground && git pull\n",
    "    %cd /content/ttt-playground\n",
    "else:\n",
    "    !git clone https://github.com/sivaratrisrinivas/ttt-playground.git\n",
    "    %cd ttt-playground\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "print(f\"✓ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all imports\n",
    "import torch\n",
    "import transformers\n",
    "import fitz  # PyMuPDF\n",
    "import gradio\n",
    "import tiktoken\n",
    "import tqdm\n",
    "from loguru import logger\n",
    "import pydantic\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Phase 2: Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import document processing modules\n",
    "from src.document.pdf_parser import PDFParser, PDFExtractionError\n",
    "from src.document.chunker import DocumentChunker\n",
    "from src.document.validator import DocumentValidator\n",
    "from src.config import DocumentConstraints, DocumentChunk\n",
    "from transformers import AutoTokenizer\n",
    "import fitz\n",
    "print(\"✓ Document processing imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Generate Test PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_pdf(filename: str, num_pages: int, text_per_page: str):\n",
    "    \"\"\"Create a test PDF with specified pages and text\"\"\"\n",
    "    doc = fitz.open()\n",
    "    for i in range(num_pages):\n",
    "        page = doc.new_page()\n",
    "        page.insert_text((50, 50), f\"Page {i+1}\")\n",
    "        page.insert_text((50, 100), text_per_page)\n",
    "    doc.save(filename)\n",
    "    doc.close()\n",
    "    print(f\"Created {filename} ({num_pages} pages)\")\n",
    "\n",
    "text_short = \"This is a short test document. \" * 50\n",
    "create_test_pdf(\"test_short.pdf\", 3, text_short)\n",
    "\n",
    "text_medium = \"This is a medium test document with more content. \" * 100\n",
    "create_test_pdf(\"test_medium.pdf\", 20, text_medium)\n",
    "\n",
    "with open(\"test_corrupt.pdf\", \"wb\") as f:\n",
    "    f.write(b\"not a valid pdf file\")\n",
    "\n",
    "print(\"\\n✓ Test PDFs created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2-2.3 PDFParser Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PDFParser()\n",
    "\n",
    "# Test valid PDF\n",
    "with open(\"test_short.pdf\", \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "text, page_count = parser.parse(pdf_bytes)\n",
    "print(f\"✓ Parsed test_short.pdf:\")\n",
    "print(f\"  - Pages: {page_count}\")\n",
    "print(f\"  - Text length: {len(text)} chars\")\n",
    "assert page_count > 0 and len(text) > 0\n",
    "\n",
    "# Test error handling\n",
    "try:\n",
    "    with open(\"test_corrupt.pdf\", \"rb\") as f:\n",
    "        parser.parse(f.read())\n",
    "    assert False, \"Should have raised PDFExtractionError\"\n",
    "except PDFExtractionError:\n",
    "    print(\"✓ Error handling works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4-2.6 DocumentChunker Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "chunker = DocumentChunker(tokenizer, chunk_size=2048)\n",
    "print(f\"✓ Chunker initialized with chunk_size={chunker.chunk_size}\")\n",
    "\n",
    "# Test short text (single chunk)\n",
    "short_text = \"This is a short text. \" * 10\n",
    "chunks_short = chunker.chunk(short_text)\n",
    "print(f\"✓ Short text: {len(chunks_short)} chunk(s)\")\n",
    "\n",
    "# Test large text (multiple chunks)\n",
    "large_text = \"word \" * 5000\n",
    "chunks_large = chunker.chunk(large_text)\n",
    "print(f\"✓ Large text (~5000 tokens): {len(chunks_large)} chunks\")\n",
    "for i, chunk in enumerate(chunks_large):\n",
    "    assert chunk.token_count <= 2048, f\"Chunk {i} exceeds limit\"\n",
    "\n",
    "# Verify token preservation\n",
    "original_ids = tokenizer.encode(large_text, add_special_tokens=False)\n",
    "reconstructed_ids = []\n",
    "for chunk in chunks_large:\n",
    "    reconstructed_ids.extend(chunk.token_ids)\n",
    "assert reconstructed_ids == original_ids, \"Token preservation failed!\"\n",
    "print(f\"✓ Token preservation verified: {len(original_ids)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 DocumentValidator Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = DocumentValidator()\n",
    "\n",
    "with open(\"test_short.pdf\", \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "# Test valid (relaxed constraints)\n",
    "is_valid, _ = validator.validate(pdf_bytes, DocumentConstraints(min_tokens=50))\n",
    "assert is_valid, \"Should pass relaxed validation\"\n",
    "print(\"✓ Valid PDF passes\")\n",
    "\n",
    "# Test max_pages violation\n",
    "is_valid, msg = validator.validate(pdf_bytes, DocumentConstraints(max_pages=2, min_tokens=50))\n",
    "assert not is_valid\n",
    "print(f\"✓ max_pages violation detected: {msg}\")\n",
    "\n",
    "# Test min_tokens violation\n",
    "is_valid, msg = validator.validate(pdf_bytes, DocumentConstraints(min_tokens=500))\n",
    "assert not is_valid\n",
    "print(f\"✓ min_tokens violation detected: {msg}\")\n",
    "\n",
    "# Test corrupt PDF\n",
    "with open(\"test_corrupt.pdf\", \"rb\") as f:\n",
    "    is_valid, msg = validator.validate(f.read(), DocumentConstraints(min_tokens=50))\n",
    "assert not is_valid\n",
    "print(f\"✓ Corrupt PDF rejected: {msg}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✓ ALL PHASE 2 TESTS PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Phase 3: TTT-Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import models package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import *\n",
    "print(\"✓ Step 3.1: from src.models import * succeeds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 TTTLinear.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.ttt_linear import TTTLinear\n",
    "\n",
    "layer = TTTLinear(768, 2048, 768)\n",
    "print(f\"✓ TTTLinear instantiated\")\n",
    "print(f\"  W_h.shape: {layer.W_h.shape}\")\n",
    "assert layer.W_h.shape == (2048, 768)\n",
    "print(\"✓ Step 3.2: W_h.shape == (2048, 768) verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 TTTLinear.forward (inference mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1, 128, 768)\n",
    "y = layer(x, learning=False)\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Output shape: {y.shape}\")\n",
    "assert y.shape == (1, 128, 768)\n",
    "print(\"✓ Step 3.3: Output shape [1, 128, 768] verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Initial weights stored for reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hasattr(layer, '_W_h_initial'), \"Missing _W_h_initial attribute\"\n",
    "assert torch.allclose(layer.W_h, layer._W_h_initial)\n",
    "print(\"✓ Step 3.4: _W_h_initial stored and matches W_h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 TTTLinear.forward (learning mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TTTLinear(768, 2048, 768)\n",
    "w_before = layer.W_h.clone()\n",
    "\n",
    "x = torch.randn(1, 128, 768)\n",
    "y = layer(x, learning=True)\n",
    "\n",
    "assert not torch.allclose(layer.W_h, w_before), \"W_h should change after learning=True\"\n",
    "print(\"✓ Step 3.5: W_h differs from initial after learning=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 reset_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.reset_weights()\n",
    "assert torch.allclose(layer.W_h, layer._W_h_initial)\n",
    "print(\"✓ Step 3.6: reset_weights() restores initial W_h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 get_weight_delta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TTTLinear(768, 2048, 768)\n",
    "x = torch.randn(1, 128, 768)\n",
    "layer(x, learning=True)\n",
    "\n",
    "delta = layer.get_weight_delta()\n",
    "print(f\"  Weight delta: {delta}\")\n",
    "assert delta > 0\n",
    "print(\"✓ Step 3.7: get_weight_delta() > 0 after learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TTTLinear(768, 2048, 768)\n",
    "x = torch.randn(1, 128, 768, requires_grad=True)\n",
    "y = layer(x, learning=False)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "assert x.grad is not None\n",
    "print(\"✓ Step 3.8: Gradient flows through layer\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✓ ALL PHASE 3 TESTS PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
